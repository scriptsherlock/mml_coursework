{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "possible_n_vals = [10, 12, 14, 16]\n",
    "possible_e_vals = [1, 2, 3, 4, 5]\n",
    "\n",
    "\n",
    "def run_poly_logistic_regression(n, e):\n",
    "\n",
    "    X = np.load('Datasets/kryptonite-%s-X.npy'%(n))\n",
    "    y = np.load('Datasets/kryptonite-%s-y.npy'%(n))\n",
    "\n",
    "    # Shuffle and split the data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.6, random_state=42)  # 60% training\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 20% validation, 20% test\n",
    "    print(X_train.shape)\n",
    "    # Create polynomial features (set degree as desired)\n",
    "    degree = e\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    print(X_train_poly.shape)\n",
    "    features = X_train_poly.shape[-1]\n",
    "    print(\"Created features\")\n",
    "\n",
    "    # Initialize and fit logistic regression\n",
    "    logreg = LogisticRegression(max_iter=100, solver='sag', C=0.85)\n",
    "    logreg.fit(X_train_poly, y_train)\n",
    "    print(\"Fit Model\")\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = logreg.predict(X_val_poly)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    y_test_pred = logreg.predict(X_test_poly)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    return test_accuracy, features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30f8a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp311-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/nicholasscheurenbrand/Library/Python/3.11/lib/python/site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Downloading torch-2.9.0-cp311-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [torch]32m7/8\u001b[0m [torch]]x]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 filelock-3.20.0 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 sympy-1.14.0 torch-2.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "acc_by_n = []\n",
    "feat_by_n = []\n",
    "for n in tqdm(possible_n_vals):\n",
    "    single_n = []\n",
    "    single_feat = []\n",
    "    for e in tqdm(possible_e_vals):\n",
    "        acc, feat = run_poly_logistic_regression(n, e)\n",
    "        single_n.append(acc)\n",
    "        single_feat.append(feat)\n",
    "    acc_by_n.append(single_n)\n",
    "    feat_by_n.append(single_feat)\n",
    "\n",
    "print(acc_by_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73243b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-28 19:12:30,988] A new study created in memory with name: no-name-a69ea27a-81d9-4af3-94ad-69d6d615db5f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 24000, Val: 8000, Test: 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-28 19:12:55,146] Trial 0 finished with value: 0.499125 and parameters: {'n_layers': 3, 'hidden_dim': 256, 'dropout_rate': 0.43038441293451746, 'lr': 0.00013432816576831912, 'weight_decay': 8.829359118759904e-05, 'activation': 'GELU'}. Best is trial 0 with value: 0.499125.\n",
      "[I 2025-10-28 19:13:34,318] Trial 1 finished with value: 0.505125 and parameters: {'n_layers': 5, 'hidden_dim': 256, 'dropout_rate': 0.017527353437521986, 'lr': 0.0002612559722668915, 'weight_decay': 0.0005395501137190133, 'activation': 'GELU'}. Best is trial 1 with value: 0.505125.\n",
      "[I 2025-10-28 19:15:35,786] Trial 2 finished with value: 0.94925 and parameters: {'n_layers': 3, 'hidden_dim': 1024, 'dropout_rate': 0.042366486743149634, 'lr': 0.0007384888819631559, 'weight_decay': 6.772946771940114e-06, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:15:46,468] Trial 3 finished with value: 0.649 and parameters: {'n_layers': 1, 'hidden_dim': 768, 'dropout_rate': 0.14245629527355536, 'lr': 0.0007068549932497406, 'weight_decay': 0.00041844878361814507, 'activation': 'ReLU'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:16:53,161] Trial 4 finished with value: 0.495 and parameters: {'n_layers': 2, 'hidden_dim': 1024, 'dropout_rate': 0.15775088829539363, 'lr': 0.00042594343695979037, 'weight_decay': 6.92381983998828e-05, 'activation': 'GELU'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:17:15,313] Trial 5 finished with value: 0.493625 and parameters: {'n_layers': 2, 'hidden_dim': 512, 'dropout_rate': 0.42778554591077317, 'lr': 0.00025285329040599964, 'weight_decay': 1.3701371786487074e-06, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:17:24,064] Trial 6 finished with value: 0.50625 and parameters: {'n_layers': 2, 'hidden_dim': 128, 'dropout_rate': 0.27112035348004626, 'lr': 0.0029938553237240785, 'weight_decay': 2.9112326720994265e-05, 'activation': 'GELU'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:19:08,484] Trial 7 finished with value: 0.501 and parameters: {'n_layers': 4, 'hidden_dim': 768, 'dropout_rate': 0.4195937196933084, 'lr': 0.00018416154806262237, 'weight_decay': 0.00013655086507379454, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:19:35,794] Trial 8 finished with value: 0.505125 and parameters: {'n_layers': 4, 'hidden_dim': 256, 'dropout_rate': 0.26768399525175335, 'lr': 0.00041766239415762154, 'weight_decay': 0.0002047609456933836, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:19:47,442] Trial 9 finished with value: 0.498625 and parameters: {'n_layers': 3, 'hidden_dim': 128, 'dropout_rate': 0.27749249885828603, 'lr': 0.0007621841358184706, 'weight_decay': 0.00012224225147201633, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:26:05,198] Trial 10 finished with value: 0.505125 and parameters: {'n_layers': 5, 'hidden_dim': 1024, 'dropout_rate': 0.00866550521381168, 'lr': 0.002404365682176699, 'weight_decay': 6.226094149400159e-06, 'activation': 'ReLU'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:26:16,312] Trial 11 finished with value: 0.57375 and parameters: {'n_layers': 1, 'hidden_dim': 768, 'dropout_rate': 0.12915484971049904, 'lr': 0.008421142116523565, 'weight_decay': 9.873617801067598e-06, 'activation': 'ReLU'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:26:29,541] Trial 12 finished with value: 0.677875 and parameters: {'n_layers': 1, 'hidden_dim': 1024, 'dropout_rate': 0.11254131768758002, 'lr': 0.00132916181064762, 'weight_decay': 0.0008155204632876485, 'activation': 'ReLU'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:26:42,597] Trial 13 finished with value: 0.812875 and parameters: {'n_layers': 1, 'hidden_dim': 1024, 'dropout_rate': 0.07958522980616597, 'lr': 0.0016013790482403295, 'weight_decay': 2.4703425646976186e-06, 'activation': 'ReLU'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:29:31,093] Trial 14 finished with value: 0.497875 and parameters: {'n_layers': 4, 'hidden_dim': 1024, 'dropout_rate': 0.06530185860156626, 'lr': 0.0015963093888905642, 'weight_decay': 1.0041620068314728e-06, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:30:31,187] Trial 15 finished with value: 0.49425 and parameters: {'n_layers': 2, 'hidden_dim': 1024, 'dropout_rate': 0.1970501772338307, 'lr': 0.005877175545631415, 'weight_decay': 3.991966039408337e-06, 'activation': 'ReLU'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:31:05,495] Trial 16 finished with value: 0.499125 and parameters: {'n_layers': 3, 'hidden_dim': 512, 'dropout_rate': 0.06697957201563162, 'lr': 0.0036099793281277708, 'weight_decay': 1.74035607709316e-05, 'activation': 'ReLU'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:31:18,683] Trial 17 finished with value: 0.497875 and parameters: {'n_layers': 1, 'hidden_dim': 1024, 'dropout_rate': 0.3439813663542745, 'lr': 0.0010953552710026813, 'weight_decay': 2.514328658164382e-06, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:34:11,354] Trial 18 finished with value: 0.943 and parameters: {'n_layers': 4, 'hidden_dim': 1024, 'dropout_rate': 0.1955995598229402, 'lr': 0.0019787204221120907, 'weight_decay': 2.7079854406714323e-06, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:37:01,535] Trial 19 finished with value: 0.50675 and parameters: {'n_layers': 4, 'hidden_dim': 1024, 'dropout_rate': 0.20688260582965676, 'lr': 0.0004706054439015375, 'weight_decay': 9.968818828828476e-06, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:37:12,824] Trial 20 finished with value: 0.507375 and parameters: {'n_layers': 3, 'hidden_dim': 128, 'dropout_rate': 0.352610921307808, 'lr': 0.002471859196449069, 'weight_decay': 3.34176617851589e-05, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:40:59,776] Trial 21 finished with value: 0.502375 and parameters: {'n_layers': 5, 'hidden_dim': 1024, 'dropout_rate': 0.07339330994474208, 'lr': 0.0019219043685176885, 'weight_decay': 2.86237974015455e-06, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:44:10,238] Trial 22 finished with value: 0.49825 and parameters: {'n_layers': 4, 'hidden_dim': 1024, 'dropout_rate': 0.18637142438282273, 'lr': 0.004472943267023729, 'weight_decay': 1.9921173087248676e-06, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:45:19,293] Trial 23 finished with value: 0.528875 and parameters: {'n_layers': 2, 'hidden_dim': 1024, 'dropout_rate': 0.09432555883998903, 'lr': 0.0008419566188516417, 'weight_decay': 4.335799226319131e-06, 'activation': 'ReLU'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 19:46:24,960] Trial 24 finished with value: 0.492625 and parameters: {'n_layers': 4, 'hidden_dim': 512, 'dropout_rate': 0.030016351501342797, 'lr': 0.0014794188537158753, 'weight_decay': 9.023816119522522e-06, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 20:20:03,081] Trial 25 finished with value: 0.517625 and parameters: {'n_layers': 5, 'hidden_dim': 1024, 'dropout_rate': 0.23279975659254373, 'lr': 0.0005805448649685553, 'weight_decay': 1.6624174525986373e-06, 'activation': 'ReLU'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 20:22:06,725] Trial 26 finished with value: 0.918 and parameters: {'n_layers': 3, 'hidden_dim': 1024, 'dropout_rate': 0.04615361829515393, 'lr': 0.0011531363587918566, 'weight_decay': 4.567504981975334e-06, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 20:24:09,288] Trial 27 finished with value: 0.92 and parameters: {'n_layers': 3, 'hidden_dim': 1024, 'dropout_rate': 0.4957776588632576, 'lr': 0.0009897135452590338, 'weight_decay': 2.1728732689456245e-05, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 20:26:08,249] Trial 28 finished with value: 0.91225 and parameters: {'n_layers': 3, 'hidden_dim': 1024, 'dropout_rate': 0.4968133673756465, 'lr': 0.0003206667067117805, 'weight_decay': 2.06120437192254e-05, 'activation': 'Tanh'}. Best is trial 2 with value: 0.94925.\n",
      "[I 2025-10-28 20:26:31,448] Trial 29 finished with value: 0.5005 and parameters: {'n_layers': 3, 'hidden_dim': 256, 'dropout_rate': 0.49955854979393877, 'lr': 0.00011127080133951713, 'weight_decay': 5.7887551625189716e-05, 'activation': 'GELU'}. Best is trial 2 with value: 0.94925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Validation Results\n",
      "------------------------\n",
      "Validation Accuracy: 0.9493\n",
      "  n_layers: 3\n",
      "  hidden_dim: 1024\n",
      "  dropout_rate: 0.042366486743149634\n",
      "  lr: 0.0007384888819631559\n",
      "  weight_decay: 6.772946771940114e-06\n",
      "  activation: Tanh\n",
      "\n",
      "Final Test Accuracy (using best params): 0.9621\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PyTorch + Optuna: Binary classification optimizing accuracy\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "import optuna\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. (Example) Generate or load your dataset\n",
    "# ------------------------------------------------------------\n",
    "# Replace this with your actual dataset\n",
    "n = 20\n",
    "\n",
    "X = np.load('Datasets/kryptonite-%s-X.npy'%(n))\n",
    "y = np.load('Datasets/kryptonite-%s-y.npy'%(n))\n",
    "\n",
    "# First split: train 60%, temp 40%\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=42\n",
    ")\n",
    "# Second split: val 20%, test 20%\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Preprocessing\n",
    "# ------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Define flexible MLP model\n",
    "# ------------------------------------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout_rate, activation_fn):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hdim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hdim))\n",
    "            layers.append(activation_fn())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = hdim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.net(x))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Training + Validation function\n",
    "# ------------------------------------------------------------\n",
    "def train_and_evaluate(model, optimizer, criterion,\n",
    "                       X_train, y_train, X_val, y_val,\n",
    "                       epochs=50, batch_size=64):\n",
    "    n = len(X_train)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        idx = torch.randperm(n)\n",
    "        X_train, y_train = X_train[idx], y_train[idx]\n",
    "        \n",
    "        for i in range(0, n, batch_size):\n",
    "            xb = X_train[i:i+batch_size]\n",
    "            yb = y_train[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_val = model(X_val)\n",
    "    preds_bin = (preds_val > 0.5).float()\n",
    "    acc = accuracy_score(y_val, preds_bin)\n",
    "    return acc\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Optuna Objective Function (optimize validation accuracy)\n",
    "# ------------------------------------------------------------\n",
    "def objective(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256, 512, 768, 1024])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    activation_name = trial.suggest_categorical(\"activation\", [\"ReLU\", \"Tanh\", \"GELU\"])\n",
    "    \n",
    "    activation_fn = {\"ReLU\": nn.ReLU, \"Tanh\": nn.Tanh, \"GELU\": nn.GELU}[activation_name]\n",
    "    hidden_dims = [hidden_dim] * n_layers\n",
    "    \n",
    "    model = MLP(input_dim=X_train.shape[1],\n",
    "                hidden_dims=hidden_dims,\n",
    "                dropout_rate=dropout_rate,\n",
    "                activation_fn=activation_fn)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    acc = train_and_evaluate(model, optimizer, criterion, X_train, y_train, X_val, y_val)\n",
    "    return acc\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Run Optuna optimization\n",
    "# ------------------------------------------------------------\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"\\nBest Validation Results\")\n",
    "print(\"------------------------\")\n",
    "print(f\"Validation Accuracy: {best_trial.value:.4f}\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Evaluate on Test Set using Best Params\n",
    "# ------------------------------------------------------------\n",
    "def train_full_and_test(params):\n",
    "    n_layers = params[\"n_layers\"]\n",
    "    hidden_dim = params[\"hidden_dim\"]\n",
    "    dropout_rate = params[\"dropout_rate\"]\n",
    "    lr = params[\"lr\"]\n",
    "    weight_decay = params[\"weight_decay\"]\n",
    "    activation_fn = {\"ReLU\": nn.ReLU, \"Tanh\": nn.Tanh, \"GELU\": nn.GELU}[params[\"activation\"]]\n",
    "    \n",
    "    model = MLP(input_dim=X_train.shape[1],\n",
    "                hidden_dims=[hidden_dim] * n_layers,\n",
    "                dropout_rate=dropout_rate,\n",
    "                activation_fn=activation_fn)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Train on train + val (80%) for better generalization\n",
    "    X_combined = torch.cat([X_train, X_val], dim=0)\n",
    "    y_combined = torch.cat([y_train, y_val], dim=0)\n",
    "    \n",
    "    _ = train_and_evaluate(model, optimizer, criterion, X_combined, y_combined, X_test, y_test)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_test = model(X_test)\n",
    "    preds_bin = (preds_test > 0.5).float()\n",
    "    test_acc = accuracy_score(y_test, preds_bin)\n",
    "    return test_acc\n",
    "\n",
    "test_acc = train_full_and_test(best_trial.params)\n",
    "print(\"\\nFinal Test Accuracy (using best params): {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c46628",
   "metadata": {},
   "source": [
    "### For n = 10\n",
    "Validation Accuracy: 0.9627\n",
    "\n",
    "  n_layers: 1\n",
    "  hidden_dim: 256\n",
    "  dropout_rate: 0.4027371651972238\n",
    "  lr: 0.008265555263166885\n",
    "  weight_decay: 9.054101276144208e-06\n",
    "  activation: ReLU\n",
    "\n",
    "Final Test Accuracy (using best params): 0.9615\n",
    "\n",
    "\n",
    "### For n = 12\n",
    "Validation Accuracy: 0.9606\n",
    "  n_layers: 2\n",
    "  hidden_dim: 256\n",
    "  dropout_rate: 0.29257082699124565\n",
    "  lr: 0.001426579019898136\n",
    "  weight_decay: 3.7383700968473337e-06\n",
    "  activation: Tanh\n",
    "\n",
    "Final Test Accuracy (using best params): 0.9421\n",
    "\n",
    "### For n = 14\n",
    "Validation Accuracy: 0.9646\n",
    "\n",
    "  n_layers: 2\n",
    "  hidden_dim: 256\n",
    "  dropout_rate: 0.18930988641398677\n",
    "  lr: 0.0005284962214684265\n",
    "  weight_decay: 1.8689152926603513e-05\n",
    "  activation: ReLU\n",
    "\n",
    "Final Test Accuracy (using best params): 0.9634\n",
    "\n",
    "### For n = 16\n",
    "Validation Accuracy: 0.9619\n",
    "\n",
    "  n_layers: 3\n",
    "  hidden_dim: 512\n",
    "  dropout_rate: 0.22305400020665447\n",
    "  lr: 0.0003932872869573933\n",
    "  weight_decay: 5.556403819066895e-06\n",
    "  activation: Tanh\n",
    "\n",
    "Final Test Accuracy (using best params): 0.9470\n",
    "\n",
    "### For n = 18\n",
    "Validation Accuracy: 0.9211\n",
    "\n",
    "  n_layers: 1\n",
    "  hidden_dim: 1024\n",
    "  dropout_rate: 0.007703409696213498\n",
    "  lr: 0.000777485254347479\n",
    "  weight_decay: 0.00013896378432597418\n",
    "  activation: ReLU\n",
    "\n",
    "Final Test Accuracy (using best params): 0.9471\n",
    "\n",
    "### For n = 20\n",
    "Validation Accuracy: 0.9493\n",
    "\n",
    "  n_layers: 3\n",
    "  hidden_dim: 1024\n",
    "  dropout_rate: 0.042366486743149634\n",
    "  lr: 0.0007384888819631559\n",
    "  weight_decay: 6.772946771940114e-06\n",
    "  activation: Tanh\n",
    "\n",
    "Final Test Accuracy (using best params): 0.9621\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f36c3dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-29 09:58:41,786] A new study created in memory with name: no-name-5035d61e-a35c-4f8d-a631-40fca4b32917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 12000, Val: 4000, Test: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [09:58:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 09:58:43,451] Trial 0 finished with value: 0.5165 and parameters: {'model_type': 'XGBoost', 'n_estimators': 417, 'learning_rate': 0.053670510861832735, 'max_depth': 10, 'subsample': 0.8066624939979578, 'colsample_bytree': 0.9388283744981742, 'reg_lambda': 0.15489223417538414, 'reg_alpha': 7.556749926701239}. Best is trial 0 with value: 0.5165.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [09:58:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 09:58:43,912] Trial 1 finished with value: 0.4995 and parameters: {'model_type': 'XGBoost', 'n_estimators': 298, 'learning_rate': 0.0859816760167959, 'max_depth': 5, 'subsample': 0.7700559163878782, 'colsample_bytree': 0.8736236104091871, 'reg_lambda': 2.300218330034362, 'reg_alpha': 0.024489472491352228}. Best is trial 0 with value: 0.5165.\n",
      "[I 2025-10-29 09:58:49,494] Trial 2 finished with value: 0.4975 and parameters: {'model_type': 'GradientBoosting', 'n_estimators': 242, 'learning_rate': 0.06869564901111438, 'max_depth': 3, 'subsample': 0.6530365405098738}. Best is trial 0 with value: 0.5165.\n",
      "[I 2025-10-29 09:58:57,933] Trial 3 finished with value: 0.49325 and parameters: {'model_type': 'RandomForest', 'n_estimators': 482, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 0 with value: 0.5165.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [09:58:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 09:58:58,718] Trial 4 finished with value: 0.5045 and parameters: {'model_type': 'XGBoost', 'n_estimators': 495, 'learning_rate': 0.04130610216929721, 'max_depth': 6, 'subsample': 0.7381730521184551, 'colsample_bytree': 0.6564207318421785, 'reg_lambda': 0.004894569878518483, 'reg_alpha': 0.051552908386194994}. Best is trial 0 with value: 0.5165.\n",
      "[I 2025-10-29 09:59:02,669] Trial 5 finished with value: 0.4945 and parameters: {'model_type': 'RandomForest', 'n_estimators': 316, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 0 with value: 0.5165.\n",
      "[I 2025-10-29 09:59:09,534] Trial 6 finished with value: 0.50375 and parameters: {'model_type': 'RandomForest', 'n_estimators': 277, 'max_depth': 18, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': None}. Best is trial 0 with value: 0.5165.\n",
      "[I 2025-10-29 09:59:49,040] Trial 7 finished with value: 0.49725 and parameters: {'model_type': 'GradientBoosting', 'n_estimators': 472, 'learning_rate': 0.06066215627731386, 'max_depth': 8, 'subsample': 0.9215429928819087}. Best is trial 0 with value: 0.5165.\n",
      "[I 2025-10-29 10:00:00,113] Trial 8 finished with value: 0.49175 and parameters: {'model_type': 'GradientBoosting', 'n_estimators': 166, 'learning_rate': 0.05434801755191453, 'max_depth': 8, 'subsample': 0.7506709545693286}. Best is trial 0 with value: 0.5165.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:00,428] Trial 9 finished with value: 0.4915 and parameters: {'model_type': 'XGBoost', 'n_estimators': 113, 'learning_rate': 0.030010536770225832, 'max_depth': 9, 'subsample': 0.9273447198741136, 'colsample_bytree': 0.64596207180939, 'reg_lambda': 1.4095693859019347, 'reg_alpha': 0.008411947907180651}. Best is trial 0 with value: 0.5165.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:01,480] Trial 10 finished with value: 0.51875 and parameters: {'model_type': 'XGBoost', 'n_estimators': 391, 'learning_rate': 0.24899237903722532, 'max_depth': 9, 'subsample': 0.8624574554760971, 'colsample_bytree': 0.9990673860710776, 'reg_lambda': 0.05251187507161141, 'reg_alpha': 8.281107651357745}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:02,615] Trial 11 finished with value: 0.49075 and parameters: {'model_type': 'XGBoost', 'n_estimators': 398, 'learning_rate': 0.2691320691898817, 'max_depth': 10, 'subsample': 0.8557123653854025, 'colsample_bytree': 0.9818294804977654, 'reg_lambda': 0.03563773604432395, 'reg_alpha': 7.965276623618629}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:03,738] Trial 12 finished with value: 0.51425 and parameters: {'model_type': 'XGBoost', 'n_estimators': 385, 'learning_rate': 0.22168273583748377, 'max_depth': 10, 'subsample': 0.8371627896044358, 'colsample_bytree': 0.9832922727816538, 'reg_lambda': 0.08210462335894524, 'reg_alpha': 8.895999106895935}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:04,925] Trial 13 finished with value: 0.4955 and parameters: {'model_type': 'XGBoost', 'n_estimators': 383, 'learning_rate': 0.15426612122992822, 'max_depth': 9, 'subsample': 0.9816601902941147, 'colsample_bytree': 0.8790828987241983, 'reg_lambda': 0.1830457873377842, 'reg_alpha': 0.8215280047331701}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:06,403] Trial 14 finished with value: 0.51175 and parameters: {'model_type': 'XGBoost', 'n_estimators': 427, 'learning_rate': 0.1611273941953788, 'max_depth': 10, 'subsample': 0.8666050108562888, 'colsample_bytree': 0.9990165963972432, 'reg_lambda': 0.010997754452736465, 'reg_alpha': 0.6292673248343469}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:07,273] Trial 15 finished with value: 0.49125 and parameters: {'model_type': 'XGBoost', 'n_estimators': 346, 'learning_rate': 0.28689745083403834, 'max_depth': 8, 'subsample': 0.6732477895268975, 'colsample_bytree': 0.8800210785098886, 'reg_lambda': 0.326529367781127, 'reg_alpha': 1.3752655138076841}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:08,428] Trial 16 finished with value: 0.4975 and parameters: {'model_type': 'XGBoost', 'n_estimators': 437, 'learning_rate': 0.12813364850061215, 'max_depth': 9, 'subsample': 0.804724576267982, 'colsample_bytree': 0.7621627715169532, 'reg_lambda': 0.001661754894137949, 'reg_alpha': 0.0017672418505637764}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:09,202] Trial 17 finished with value: 0.4975 and parameters: {'model_type': 'XGBoost', 'n_estimators': 353, 'learning_rate': 0.2121118993114298, 'max_depth': 7, 'subsample': 0.9037003712199362, 'colsample_bytree': 0.9269975330717215, 'reg_lambda': 0.47850854609735244, 'reg_alpha': 2.2242722256787313}. Best is trial 10 with value: 0.51875.\n",
      "[I 2025-10-29 10:00:10,841] Trial 18 finished with value: 0.4875 and parameters: {'model_type': 'RandomForest', 'n_estimators': 236, 'max_depth': 15, 'min_samples_split': 10, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.51875.\n",
      "[I 2025-10-29 10:00:28,092] Trial 19 finished with value: 0.4915 and parameters: {'model_type': 'GradientBoosting', 'n_estimators': 437, 'learning_rate': 0.2261787215607367, 'max_depth': 5, 'subsample': 0.7010448619047701}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:29,048] Trial 20 finished with value: 0.49875 and parameters: {'model_type': 'XGBoost', 'n_estimators': 336, 'learning_rate': 0.10825252438083646, 'max_depth': 9, 'subsample': 0.8069249578195545, 'colsample_bytree': 0.773479483451814, 'reg_lambda': 0.03306488496499958, 'reg_alpha': 0.2834655315296947}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:30,238] Trial 21 finished with value: 0.50925 and parameters: {'model_type': 'XGBoost', 'n_estimators': 394, 'learning_rate': 0.2225473429472452, 'max_depth': 10, 'subsample': 0.8529467132258509, 'colsample_bytree': 0.943902135329073, 'reg_lambda': 0.07145192902178216, 'reg_alpha': 9.116733911107344}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:31,443] Trial 22 finished with value: 0.51275 and parameters: {'model_type': 'XGBoost', 'n_estimators': 384, 'learning_rate': 0.2626458665562863, 'max_depth': 10, 'subsample': 0.8193279983794612, 'colsample_bytree': 0.9458465445956149, 'reg_lambda': 0.09415600896528548, 'reg_alpha': 3.9452023958820277}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:32,664] Trial 23 finished with value: 0.513 and parameters: {'model_type': 'XGBoost', 'n_estimators': 449, 'learning_rate': 0.18833524626637754, 'max_depth': 9, 'subsample': 0.8831802934924964, 'colsample_bytree': 0.9984683207436781, 'reg_lambda': 0.01882745682026106, 'reg_alpha': 8.553977507075757}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:34,031] Trial 24 finished with value: 0.517 and parameters: {'model_type': 'XGBoost', 'n_estimators': 408, 'learning_rate': 0.25067076239700703, 'max_depth': 10, 'subsample': 0.95604815079865, 'colsample_bytree': 0.9324076128035805, 'reg_lambda': 0.6689042367541048, 'reg_alpha': 2.775120407119435}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:35,147] Trial 25 finished with value: 0.49225 and parameters: {'model_type': 'XGBoost', 'n_estimators': 428, 'learning_rate': 0.2544772726130337, 'max_depth': 8, 'subsample': 0.9918011794439157, 'colsample_bytree': 0.8333043098994335, 'reg_lambda': 8.80123045778997, 'reg_alpha': 0.18245061478210242}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:36,500] Trial 26 finished with value: 0.511 and parameters: {'model_type': 'XGBoost', 'n_estimators': 458, 'learning_rate': 0.29174194604540826, 'max_depth': 9, 'subsample': 0.6119107493611702, 'colsample_bytree': 0.9198566676631176, 'reg_lambda': 0.8562377064842444, 'reg_alpha': 2.624409704882173}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:00:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:00:36,903] Trial 27 finished with value: 0.492 and parameters: {'model_type': 'XGBoost', 'n_estimators': 411, 'learning_rate': 0.1850430690183627, 'max_depth': 3, 'subsample': 0.9521746320468296, 'colsample_bytree': 0.8424748936004239, 'reg_lambda': 3.741262391796148, 'reg_alpha': 3.116585168293237}. Best is trial 10 with value: 0.51875.\n",
      "[I 2025-10-29 10:01:19,952] Trial 28 finished with value: 0.5 and parameters: {'model_type': 'GradientBoosting', 'n_estimators': 361, 'learning_rate': 0.012726091829129121, 'max_depth': 10, 'subsample': 0.9596416948653403}. Best is trial 10 with value: 0.51875.\n",
      "[I 2025-10-29 10:01:21,719] Trial 29 finished with value: 0.50025 and parameters: {'model_type': 'RandomForest', 'n_estimators': 284, 'max_depth': 13, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:01:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:01:22,063] Trial 30 finished with value: 0.4985 and parameters: {'model_type': 'XGBoost', 'n_estimators': 308, 'learning_rate': 0.13363448367481928, 'max_depth': 4, 'subsample': 0.7748280069548372, 'colsample_bytree': 0.7011137148772244, 'reg_lambda': 0.3688612560250452, 'reg_alpha': 0.5038891768607892}. Best is trial 10 with value: 0.51875.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:01:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:01:23,257] Trial 31 finished with value: 0.53925 and parameters: {'model_type': 'XGBoost', 'n_estimators': 376, 'learning_rate': 0.22644178044161917, 'max_depth': 10, 'subsample': 0.836599052260243, 'colsample_bytree': 0.960831543984434, 'reg_lambda': 0.1302571317546935, 'reg_alpha': 4.644859724356098}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:01:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:01:24,422] Trial 32 finished with value: 0.51975 and parameters: {'model_type': 'XGBoost', 'n_estimators': 366, 'learning_rate': 0.24680946958991926, 'max_depth': 10, 'subsample': 0.8852305515862348, 'colsample_bytree': 0.9531492649971143, 'reg_lambda': 0.20512684456071034, 'reg_alpha': 3.811065869578441}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:01:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:01:25,497] Trial 33 finished with value: 0.50175 and parameters: {'model_type': 'XGBoost', 'n_estimators': 370, 'learning_rate': 0.2527519523011591, 'max_depth': 9, 'subsample': 0.8994315441834695, 'colsample_bytree': 0.9614705063029996, 'reg_lambda': 0.6719826860299758, 'reg_alpha': 2.0097817130449043}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:01:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:01:26,631] Trial 34 finished with value: 0.49675 and parameters: {'model_type': 'XGBoost', 'n_estimators': 335, 'learning_rate': 0.24139082247951124, 'max_depth': 10, 'subsample': 0.9287216885103041, 'colsample_bytree': 0.9082552187773415, 'reg_lambda': 0.19044483185438893, 'reg_alpha': 1.1013306407108814}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:01:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:01:27,815] Trial 35 finished with value: 0.4945 and parameters: {'model_type': 'XGBoost', 'n_estimators': 414, 'learning_rate': 0.27600764801412747, 'max_depth': 8, 'subsample': 0.8794518714590036, 'colsample_bytree': 0.9633280980632072, 'reg_lambda': 0.04541583333894204, 'reg_alpha': 4.46565814457432}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:01:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:01:28,419] Trial 36 finished with value: 0.49725 and parameters: {'model_type': 'XGBoost', 'n_estimators': 257, 'learning_rate': 0.19400553821871036, 'max_depth': 6, 'subsample': 0.8325498946639251, 'colsample_bytree': 0.9026593575594541, 'reg_lambda': 0.26908836976915224, 'reg_alpha': 3.666845164624276}. Best is trial 31 with value: 0.53925.\n",
      "[I 2025-10-29 10:01:30,751] Trial 37 finished with value: 0.49725 and parameters: {'model_type': 'RandomForest', 'n_estimators': 326, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 31 with value: 0.53925.\n",
      "[I 2025-10-29 10:02:21,741] Trial 38 finished with value: 0.50625 and parameters: {'model_type': 'GradientBoosting', 'n_estimators': 472, 'learning_rate': 0.24077614971179562, 'max_depth': 10, 'subsample': 0.9466831408236648}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:02:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:02:22,516] Trial 39 finished with value: 0.491 and parameters: {'model_type': 'XGBoost', 'n_estimators': 368, 'learning_rate': 0.2967829862388671, 'max_depth': 7, 'subsample': 0.9007341237220736, 'colsample_bytree': 0.9703184828017174, 'reg_lambda': 0.014309151521101994, 'reg_alpha': 1.2038966108181488}. Best is trial 31 with value: 0.53925.\n",
      "[I 2025-10-29 10:02:25,114] Trial 40 finished with value: 0.5005 and parameters: {'model_type': 'RandomForest', 'n_estimators': 408, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:02:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:02:26,419] Trial 41 finished with value: 0.52275 and parameters: {'model_type': 'XGBoost', 'n_estimators': 456, 'learning_rate': 0.20405443519210514, 'max_depth': 9, 'subsample': 0.7752051784871863, 'colsample_bytree': 0.9457310735909775, 'reg_lambda': 0.14123283702187503, 'reg_alpha': 4.980281268715982}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:02:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:02:28,044] Trial 42 finished with value: 0.512 and parameters: {'model_type': 'XGBoost', 'n_estimators': 492, 'learning_rate': 0.20397975564049253, 'max_depth': 9, 'subsample': 0.7843546855794108, 'colsample_bytree': 0.9577665346856266, 'reg_lambda': 0.10914522558616262, 'reg_alpha': 4.454659367681733}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:02:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:02:29,652] Trial 43 finished with value: 0.503 and parameters: {'model_type': 'XGBoost', 'n_estimators': 463, 'learning_rate': 0.23357987096763827, 'max_depth': 9, 'subsample': 0.744830962317277, 'colsample_bytree': 0.9066096647496065, 'reg_lambda': 0.9658778326223814, 'reg_alpha': 1.907362610600689}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:02:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:02:31,033] Trial 44 finished with value: 0.5115 and parameters: {'model_type': 'XGBoost', 'n_estimators': 500, 'learning_rate': 0.1748462208074651, 'max_depth': 9, 'subsample': 0.8376402516873497, 'colsample_bytree': 0.938146522963389, 'reg_lambda': 0.17994616816277414, 'reg_alpha': 5.023980456721943}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:02:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:02:32,110] Trial 45 finished with value: 0.50175 and parameters: {'model_type': 'XGBoost', 'n_estimators': 443, 'learning_rate': 0.2483972943452172, 'max_depth': 8, 'subsample': 0.7264541984325644, 'colsample_bytree': 0.9978351407837793, 'reg_lambda': 0.05784553952573904, 'reg_alpha': 5.259267501737165}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:02:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:02:33,410] Trial 46 finished with value: 0.496 and parameters: {'model_type': 'XGBoost', 'n_estimators': 399, 'learning_rate': 0.2053888340576294, 'max_depth': 10, 'subsample': 0.7925277584544973, 'colsample_bytree': 0.9307850135542858, 'reg_lambda': 0.13402069554168944, 'reg_alpha': 0.3655557014541635}. Best is trial 31 with value: 0.53925.\n",
      "[I 2025-10-29 10:03:08,884] Trial 47 finished with value: 0.50525 and parameters: {'model_type': 'GradientBoosting', 'n_estimators': 420, 'learning_rate': 0.2713996016699099, 'max_depth': 10, 'subsample': 0.7603058886916673}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:03:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:03:09,831] Trial 48 finished with value: 0.50225 and parameters: {'model_type': 'XGBoost', 'n_estimators': 375, 'learning_rate': 0.21742458888812832, 'max_depth': 8, 'subsample': 0.8658256515780876, 'colsample_bytree': 0.8627462110400466, 'reg_lambda': 2.3137371854921582, 'reg_alpha': 2.15914234247084}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:03:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-10-29 10:03:10,404] Trial 49 finished with value: 0.50325 and parameters: {'model_type': 'XGBoost', 'n_estimators': 205, 'learning_rate': 0.23384619988090793, 'max_depth': 9, 'subsample': 0.971994155118582, 'colsample_bytree': 0.9695911851256259, 'reg_lambda': 0.02456432837636437, 'reg_alpha': 0.11429088866904623}. Best is trial 31 with value: 0.53925.\n",
      "/opt/homebrew/lib/python3.11/site-packages/xgboost/training.py:199: UserWarning: [10:03:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Validation Results\n",
      "------------------------\n",
      "Validation Accuracy: 0.5393\n",
      "  model_type: XGBoost\n",
      "  n_estimators: 376\n",
      "  learning_rate: 0.22644178044161917\n",
      "  max_depth: 10\n",
      "  subsample: 0.836599052260243\n",
      "  colsample_bytree: 0.960831543984434\n",
      "  reg_lambda: 0.1302571317546935\n",
      "  reg_alpha: 4.644859724356098\n",
      "\n",
      "Final Test Accuracy (using best params): 0.5387\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Tree-based models (RF, GB, XGB) + Optuna hyperparameter tuning\n",
    "# Optimizing validation accuracy with 60/20/20 split\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import optuna\n",
    "\n",
    "# Optional: XGBoost (comment out if not installed)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Create or load your dataset\n",
    "# ------------------------------------------------------------\n",
    "n = 10\n",
    "\n",
    "X = np.load('Datasets/kryptonite-%s-X.npy'%(n))\n",
    "y = np.load('Datasets/kryptonite-%s-y.npy'%(n))\n",
    "\n",
    "# 60/20/20 split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Scale numeric features\n",
    "# ------------------------------------------------------------\n",
    "# Tree models are generally scale-invariant, but scaling helps consistency.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Optuna Objective Function\n",
    "# ------------------------------------------------------------\n",
    "def objective(trial):\n",
    "    model_type = trial.suggest_categorical(\"model_type\", [\"RandomForest\", \"GradientBoosting\"] + ([\"XGBoost\"] if HAS_XGB else []))\n",
    "\n",
    "    if model_type == \"RandomForest\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    elif model_type == \"GradientBoosting\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_type == \"XGBoost\" and HAS_XGB:\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
    "        reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-3, 10, log=True)\n",
    "        reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-3, 10, log=True)\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_lambda=reg_lambda,\n",
    "            reg_alpha=reg_alpha,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "    # Train and validate\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_val = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, preds_val)\n",
    "    return acc\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Run Optuna Study\n",
    "# ------------------------------------------------------------\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"\\nBest Validation Results\")\n",
    "print(\"------------------------\")\n",
    "print(f\"Validation Accuracy: {best_trial.value:.4f}\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Retrain Best Model on Train + Val, Evaluate on Test Set\n",
    "# ------------------------------------------------------------\n",
    "def train_full_and_test(params):\n",
    "    model_type = params[\"model_type\"]\n",
    "\n",
    "    if model_type == \"RandomForest\":\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=params[\"n_estimators\"],\n",
    "            max_depth=params[\"max_depth\"],\n",
    "            min_samples_split=params[\"min_samples_split\"],\n",
    "            min_samples_leaf=params[\"min_samples_leaf\"],\n",
    "            max_features=params[\"max_features\"],\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    elif model_type == \"GradientBoosting\":\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=params[\"n_estimators\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            max_depth=params[\"max_depth\"],\n",
    "            subsample=params[\"subsample\"],\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_type == \"XGBoost\" and HAS_XGB:\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=params[\"n_estimators\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            max_depth=params[\"max_depth\"],\n",
    "            subsample=params[\"subsample\"],\n",
    "            colsample_bytree=params[\"colsample_bytree\"],\n",
    "            reg_lambda=params[\"reg_lambda\"],\n",
    "            reg_alpha=params[\"reg_alpha\"],\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "    # Retrain on 80% (train + val)\n",
    "    X_combined = np.vstack([X_train, X_val])\n",
    "    y_combined = np.concatenate([y_train, y_val])\n",
    "    model.fit(X_combined, y_combined)\n",
    "\n",
    "    preds_test = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, preds_test)\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "test_acc = train_full_and_test(best_trial.params)\n",
    "print(\"\\nFinal Test Accuracy (using best params): {:.4f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf1b34",
   "metadata": {},
   "source": [
    "### n = 10\n",
    "Validation Accuracy: 0.9483\n",
    "\n",
    "  kernel: rbf\n",
    "  C: 65.70854569546646\n",
    "  gamma: auto\n",
    "\n",
    "Final Test Accuracy (using best params): 0.9505\n",
    "\n",
    "### n = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e84ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 12000, Val: 4000, Test: 4000\n",
      "SGD          → Validation Accuracy: 0.5002\n",
      "SGD+Momentum → Validation Accuracy: 0.4968\n",
      "Adam         → Validation Accuracy: 0.4968\n",
      "RMSprop      → Validation Accuracy: 0.4955\n",
      "LBFGS        → Validation Accuracy: 0.4950\n",
      "\n",
      "Best Optimizer: SGD\n",
      "\n",
      "Final Test Accuracy (using SGD): 0.5180\n",
      "\n",
      "=== Optimizer Comparison Summary ===\n",
      "SGD          → Val Accuracy: 0.5002\n",
      "SGD+Momentum → Val Accuracy: 0.4968\n",
      "Adam         → Val Accuracy: 0.4968\n",
      "RMSprop      → Val Accuracy: 0.4955\n",
      "LBFGS        → Val Accuracy: 0.4950\n",
      "\n",
      "Best Optimizer: SGD | Test Accuracy: 0.5180\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Logistic Regression (PyTorch) + Multiple Optimizers\n",
    "# Comparing SGD, SGD+Momentum, Adam, RMSprop, LBFGS\n",
    "# with 60/20/20 train/val/test split\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. (Example) Create or load your dataset\n",
    "# ------------------------------------------------------------\n",
    "n = 10\n",
    "\n",
    "X = np.load('Datasets/kryptonite-%s-X.npy'%(n))\n",
    "y = np.load('Datasets/kryptonite-%s-y.npy'%(n))\n",
    "\n",
    "# 60 / 20 / 20 split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Preprocessing\n",
    "# ------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Logistic Regression Model\n",
    "# ------------------------------------------------------------\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Training & Evaluation Function\n",
    "# ------------------------------------------------------------\n",
    "def train_model(optimizer_name, model, X_train, y_train, X_val, y_val,\n",
    "                lr=1e-3, weight_decay=0.0, momentum=0.9, epochs=50, batch_size=64):\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Initialize optimizer\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"SGD+Momentum\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"LBFGS\":\n",
    "        optimizer = optim.LBFGS(model.parameters(), lr=lr, max_iter=20)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown optimizer\")\n",
    "\n",
    "    n = len(X_train)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        idx = torch.randperm(n)\n",
    "        X_train, y_train = X_train[idx], y_train[idx]\n",
    "\n",
    "        if optimizer_name == \"LBFGS\":\n",
    "            # LBFGS requires closure\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(X_train)\n",
    "                loss = criterion(preds, y_train)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "        else:\n",
    "            for i in range(0, n, batch_size):\n",
    "                xb = X_train[i:i+batch_size]\n",
    "                yb = y_train[i:i+batch_size]\n",
    "                optimizer.zero_grad()\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    # Validation accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_val = model(X_val)\n",
    "    preds_bin = (preds_val > 0.5).float()\n",
    "    acc = accuracy_score(y_val, preds_bin)\n",
    "    return acc\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Compare Optimizers\n",
    "# ------------------------------------------------------------\n",
    "optimizers = [\"SGD\", \"SGD+Momentum\", \"Adam\", \"RMSprop\", \"LBFGS\"]\n",
    "results = {}\n",
    "\n",
    "for opt in optimizers:\n",
    "    model = LogisticRegressionModel(X_train.shape[1])\n",
    "    acc = train_model(opt, model, X_train, y_train, X_val, y_val,\n",
    "                      lr=1e-3 if opt != \"LBFGS\" else 1.0,\n",
    "                      weight_decay=1e-4)\n",
    "    results[opt] = acc\n",
    "    print(f\"{opt:12s} → Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Select Best Optimizer & Test\n",
    "# ------------------------------------------------------------\n",
    "best_opt = max(results, key=results.get)\n",
    "print(\"\\nBest Optimizer:\", best_opt)\n",
    "\n",
    "# Retrain best model on Train + Val, Evaluate on Test\n",
    "model_best = LogisticRegressionModel(X_train.shape[1])\n",
    "X_combined = torch.cat([X_train, X_val], dim=0)\n",
    "y_combined = torch.cat([y_train, y_val], dim=0)\n",
    "\n",
    "_ = train_model(best_opt, model_best, X_combined, y_combined, X_test, y_test,\n",
    "                lr=1e-3 if best_opt != \"LBFGS\" else 1.0, weight_decay=1e-4)\n",
    "\n",
    "model_best.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test = model_best(X_test)\n",
    "preds_bin = (preds_test > 0.5).float()\n",
    "test_acc = accuracy_score(y_test, preds_bin)\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy (using {best_opt}): {test_acc:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Summary\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n=== Optimizer Comparison Summary ===\")\n",
    "for opt, acc in results.items():\n",
    "    print(f\"{opt:12s} → Val Accuracy: {acc:.4f}\")\n",
    "print(f\"\\nBest Optimizer: {best_opt} | Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062dbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-10-29 15:43:26,642] A new study created in memory with name: no-name-a383fb3c-8a4e-4291-aef8-d7704d232aaf\n",
      "/var/folders/ty/8nqfhpkn0kg6cq1bbjyvmlgr0000gn/T/ipykernel_97654/3157641241.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  C = trial.suggest_loguniform(\"C\", 1e-4, 1e3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 12000, Val: 4000, Test: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-10-29 15:43:50,030] Trial 0 failed with parameters: {'degree': 3, 'C': 2.658848704188497, 'penalty': 'l1'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/ty/8nqfhpkn0kg6cq1bbjyvmlgr0000gn/T/ipykernel_97654/3157641241.py\", line 55, in objective\n",
      "    model.fit(X_train, y_train)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1384, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 82, in __call__\n",
      "    return super().__call__(iterable_with_config_and_warning_filters)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/joblib/parallel.py\", line 1986, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/joblib/parallel.py\", line 1914, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 147, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 560, in _logistic_regression_path\n",
      "    w0, n_iter_i, warm_start_sag = sag_solver(\n",
      "                                   ^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_sag.py\", line 323, in sag_solver\n",
      "    num_seen, n_iter_ = sag(\n",
      "                        ^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-29 15:43:50,034] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# 3. Run Optuna optimization\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m     63\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m best_trial = study.best_trial\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest Validation Results\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/optuna/study/_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Build pipeline: Polynomial → Standardize → Logistic Regression\u001b[39;00m\n\u001b[32m     44\u001b[39m model = Pipeline([\n\u001b[32m     45\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mpoly\u001b[39m\u001b[33m\"\u001b[39m, PolynomialFeatures(degree=degree, include_bias=\u001b[38;5;28;01mFalse\u001b[39;00m)),\n\u001b[32m     46\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mscaler\u001b[39m\u001b[33m\"\u001b[39m, StandardScaler()),\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m         random_state=\u001b[32m42\u001b[39m))\n\u001b[32m     53\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m preds_val = model.predict(X_val)\n\u001b[32m     57\u001b[39m acc = accuracy_score(y_val, preds_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/sklearn/pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1384\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1382\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1409\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1410\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:560\u001b[39m, in \u001b[36m_logistic_regression_path\u001b[39m\u001b[34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[39m\n\u001b[32m    557\u001b[39m         alpha = (\u001b[32m1.0\u001b[39m / C) * (\u001b[32m1\u001b[39m - l1_ratio)\n\u001b[32m    558\u001b[39m         beta = (\u001b[32m1.0\u001b[39m / C) * l1_ratio\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     w0, n_iter_i, warm_start_sag = \u001b[43msag_solver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwarm_start_sag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_saga\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msolver must be one of \u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m\u001b[33mliblinear\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlbfgs\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnewton-cg\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33msag\u001b[39m\u001b[33m'\u001b[39m\u001b[33m}, got \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m instead\u001b[39m\u001b[33m\"\u001b[39m % solver\n\u001b[32m    581\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:323\u001b[39m, in \u001b[36msag_solver\u001b[39m\u001b[34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[39m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[32m    318\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCurrent sag implementation does not handle \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe case step_size * alpha_scaled == 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    322\u001b[39m sag = sag64 \u001b[38;5;28;01mif\u001b[39;00m X.dtype == np.float64 \u001b[38;5;28;01melse\u001b[39;00m sag32\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m num_seen, n_iter_ = \u001b[43msag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintercept_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43msum_gradient_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_memory_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseen_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_seen_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintercept_sum_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintercept_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_saga\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_iter_ == max_iter:\n\u001b[32m    348\u001b[39m     warnings.warn(\n\u001b[32m    349\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe max_iter was reached which means the coef_ did not converge\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    350\u001b[39m         ConvergenceWarning,\n\u001b[32m    351\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Polynomial Logistic Regression with Optuna tuning\n",
    "# Optimizes validation accuracy (60/20/20 split)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Create or load your dataset\n",
    "# ------------------------------------------------------------\n",
    "n = 10\n",
    "\n",
    "X = np.load('Datasets/kryptonite-%s-X.npy'%(n))\n",
    "y = np.load('Datasets/kryptonite-%s-y.npy'%(n))\n",
    "\n",
    "# 60/20/20 split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Define the Optuna objective\n",
    "# ------------------------------------------------------------\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    degree = trial.suggest_int(\"degree\", 1, 4)\n",
    "    C = trial.suggest_float(\"C\", 1e-4, 1e3, log=True)\n",
    "    penalty = trial.suggest_categorical(\"penalty\", [\"l2\", \"l1\"])\n",
    "    solver = \"saga\" if penalty == \"l1\" else \"lbfgs\"\n",
    "\n",
    "    # Build pipeline: Polynomial → Standardize → Logistic Regression\n",
    "    model = Pipeline([\n",
    "        (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"logreg\", LogisticRegression(\n",
    "            penalty=penalty,\n",
    "            C=C,\n",
    "            solver=solver,\n",
    "            max_iter=5000,\n",
    "            random_state=42))\n",
    "    ])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_val = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, preds_val)\n",
    "    return acc\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Run Optuna optimization\n",
    "# ------------------------------------------------------------\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"\\nBest Validation Results\")\n",
    "print(\"------------------------\")\n",
    "print(f\"Validation Accuracy: {best_trial.value:.4f}\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Retrain with best params on (Train + Val), test on Test Set\n",
    "# ------------------------------------------------------------\n",
    "def train_full_and_test(params):\n",
    "    degree = params[\"degree\"]\n",
    "    C = params[\"C\"]\n",
    "    penalty = params[\"penalty\"]\n",
    "    solver = \"saga\" if penalty == \"l1\" else \"lbfgs\"\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"logreg\", LogisticRegression(\n",
    "            penalty=penalty,\n",
    "            C=C,\n",
    "            solver=solver,\n",
    "            max_iter=5000,\n",
    "            random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Train on 80% (train + val)\n",
    "    X_combined = np.vstack([X_train, X_val])\n",
    "    y_combined = np.concatenate([y_train, y_val])\n",
    "    model.fit(X_combined, y_combined)\n",
    "\n",
    "    preds_test = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, preds_test)\n",
    "    return test_acc\n",
    "\n",
    "test_acc = train_full_and_test(best_trial.params)\n",
    "print(\"\\nFinal Test Accuracy (using best params): {:.4f}\".format(test_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
