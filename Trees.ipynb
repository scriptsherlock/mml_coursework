{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b6f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Tree-based models (RF, GB, XGB) + Optuna hyperparameter tuning\n",
    "# Optimizing validation accuracy with 60/20/20 split\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import optuna\n",
    "\n",
    "# Optional: XGBoost (comment out if not installed)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Create or load your dataset\n",
    "# ------------------------------------------------------------\n",
    "n = 10\n",
    "\n",
    "X = np.load('Datasets/kryptonite-%s-X.npy'%(n))\n",
    "y = np.load('Datasets/kryptonite-%s-y.npy'%(n))\n",
    "\n",
    "# 60/20/20 split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Scale numeric features\n",
    "# ------------------------------------------------------------\n",
    "# Tree models are generally scale-invariant, but scaling helps consistency.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Optuna Objective Function\n",
    "# ------------------------------------------------------------\n",
    "def objective(trial):\n",
    "    model_type = trial.suggest_categorical(\"model_type\", [\"RandomForest\", \"GradientBoosting\"] + ([\"XGBoost\"] if HAS_XGB else []))\n",
    "\n",
    "    if model_type == \"RandomForest\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    elif model_type == \"GradientBoosting\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_type == \"XGBoost\" and HAS_XGB:\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
    "        reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-3, 10, log=True)\n",
    "        reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-3, 10, log=True)\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_lambda=reg_lambda,\n",
    "            reg_alpha=reg_alpha,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "    # Train and validate\n",
    "    model.fit(X_train, y_train)\n",
    "    preds_val = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, preds_val)\n",
    "    return acc\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Run Optuna Study\n",
    "# ------------------------------------------------------------\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"\\nBest Validation Results\")\n",
    "print(\"------------------------\")\n",
    "print(f\"Validation Accuracy: {best_trial.value:.4f}\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Retrain Best Model on Train + Val, Evaluate on Test Set\n",
    "# ------------------------------------------------------------\n",
    "def train_full_and_test(params):\n",
    "    model_type = params[\"model_type\"]\n",
    "\n",
    "    if model_type == \"RandomForest\":\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=params[\"n_estimators\"],\n",
    "            max_depth=params[\"max_depth\"],\n",
    "            min_samples_split=params[\"min_samples_split\"],\n",
    "            min_samples_leaf=params[\"min_samples_leaf\"],\n",
    "            max_features=params[\"max_features\"],\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    elif model_type == \"GradientBoosting\":\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=params[\"n_estimators\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            max_depth=params[\"max_depth\"],\n",
    "            subsample=params[\"subsample\"],\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_type == \"XGBoost\" and HAS_XGB:\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=params[\"n_estimators\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            max_depth=params[\"max_depth\"],\n",
    "            subsample=params[\"subsample\"],\n",
    "            colsample_bytree=params[\"colsample_bytree\"],\n",
    "            reg_lambda=params[\"reg_lambda\"],\n",
    "            reg_alpha=params[\"reg_alpha\"],\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "    # Retrain on 80% (train + val)\n",
    "    X_combined = np.vstack([X_train, X_val])\n",
    "    y_combined = np.concatenate([y_train, y_val])\n",
    "    model.fit(X_combined, y_combined)\n",
    "\n",
    "    preds_test = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, preds_test)\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "test_acc = train_full_and_test(best_trial.params)\n",
    "print(\"\\nFinal Test Accuracy (using best params): {:.4f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4473b01",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
